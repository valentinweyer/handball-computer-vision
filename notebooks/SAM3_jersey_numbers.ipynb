{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading .env from: /home/valentinweyer/projects/handball-computer-vision/notebooks/.env\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Path of the current notebook file\n",
        "NOTEBOOK_DIR = Path().resolve()\n",
        "\n",
        "# Search upward for project root containing \"notebooks/.env\"\n",
        "for parent in [NOTEBOOK_DIR] + list(NOTEBOOK_DIR.parents):\n",
        "    env_candidate = parent / \"notebooks\" / \".env\"\n",
        "    if env_candidate.exists():\n",
        "        ENV_PATH = env_candidate\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\"Could not find notebooks/.env in any parent directory!\")\n",
        "\n",
        "print(\"Loading .env from:\", ENV_PATH)\n",
        "\n",
        "# Load it\n",
        "load_dotenv(ENV_PATH, override=True)\n",
        "\n",
        "# Ensure HF_TOKEN is exported\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise RuntimeError(\"HF_TOKEN not found in the .env file\")\n",
        "\n",
        "roboflow_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
        "\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"HF_TOKEN\"] = roboflow_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEqjdM5opNJG",
        "outputId": "acade69f-9ddd-4ed7-ebf7-83896388d81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Nov 27 23:04:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GB10                    On  |   0000000F:01:00.0  On |                  N/A |\n",
            "| N/A   43C    P0             11W /  N/A  | Not Supported          |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A           57489      G   /usr/bin/gnome-shell                    185MiB |\n",
            "|    0   N/A  N/A           57547      G   /usr/bin/Xwayland                         8MiB |\n",
            "|    0   N/A  N/A           97703      C   ...forge3/envs/NewEnv/bin/python       6525MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZP0wIMVpREW",
        "outputId": "2c7bfd0d-91a2-4bc5-aa32-07328b2ffc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1+cu130\n",
            "Torchvision version: 0.24.1\n",
            "CUDA is available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/valentinweyer/projects/handball-computer-vision/notebooks\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = Path.cwd()\n",
        "\n",
        "print(\"Current working directory:\", HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Python: /home/valentinweyer/miniforge3/envs/handball-computer-vision/bin/python\n",
            "[-] Uninstalling existing decord (if any)…\n",
            "Found existing installation: decord 0.6.0\n",
            "Uninstalling decord-0.6.0:\n",
            "  Successfully uninstalled decord-0.6.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping decord as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping decord as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'decord'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Temp dir: /tmp/build_decord_iy8b9ur5\n",
            "[+] Cloning decord…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Submodule '3rdparty/dlpack' (https://github.com/dmlc/dlpack) registered for path '3rdparty/dlpack'\n",
            "Submodule '3rdparty/dmlc-core' (https://github.com/dmlc/dmlc-core) registered for path '3rdparty/dmlc-core'\n",
            "Cloning into '/tmp/build_decord_iy8b9ur5/decord/3rdparty/dlpack'...\n",
            "Cloning into '/tmp/build_decord_iy8b9ur5/decord/3rdparty/dmlc-core'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submodule path '3rdparty/dlpack': checked out '5c792cef3aee54ad8b7000111c9dc1797f327b59'\n",
            "Submodule path '3rdparty/dmlc-core': checked out 'd07fb7a443b5db8a89d65a15a024af6a425615a5'\n",
            "[+] Running CMake…\n",
            "-- The C compiler identification is GNU 13.3.0\n",
            "-- The CXX compiler identification is GNU 13.3.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found PkgConfig: /usr/bin/pkg-config (found version \"1.8.1\")\n",
            "-- Checking for module 'libavcodec'\n",
            "--   Found libavcodec, version 60.31.102\n",
            "-- Checking for module 'libavformat'\n",
            "--   Found libavformat, version 60.16.100\n",
            "-- Checking for module 'libavutil'\n",
            "--   Found libavutil, version 58.29.100\n",
            "-- Checking for module 'libavdevice'\n",
            "--   Package 'libavdevice', required by 'virtual:world', not found\n",
            "-- Checking for module 'libavfilter'\n",
            "--   Found libavfilter, version 9.12.100\n",
            "-- Checking for module 'libswresample'\n",
            "--   Found libswresample, version 4.12.100\n",
            "-- Unable to find libavdevice, device input API will not work!\n",
            "-- Found FFMPEG or Libav: /usr/lib/aarch64-linux-gnu/libavformat.so;/usr/lib/aarch64-linux-gnu/libavfilter.so;/usr/lib/aarch64-linux-gnu/libavcodec.so;/usr/lib/aarch64-linux-gnu/libavutil.so;/usr/lib/aarch64-linux-gnu/libswresample.so, /usr/include/aarch64-linux-gnu\n",
            "-- The CUDA compiler identification is NVIDIA 13.0.88 with host compiler GNU 13.3.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda-13.0/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Performing Test SUPPORT_CXX11\n",
            "-- Performing Test SUPPORT_CXX11 - Success\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[0mFFMPEG_INCLUDE_DIR = /usr/include/aarch64-linux-gnu \u001b[0m\n",
            "\u001b[0mFFMPEG_LIBRARIES = /usr/lib/aarch64-linux-gnu/libavformat.so;/usr/lib/aarch64-linux-gnu/libavfilter.so;/usr/lib/aarch64-linux-gnu/libavcodec.so;/usr/lib/aarch64-linux-gnu/libavutil.so;/usr/lib/aarch64-linux-gnu/libswresample.so \u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at cmake/util/FindCUDA.cmake:43 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  cmake/modules/CUDA.cmake:19 (find_cuda)\n",
            "  CMakeLists.txt:92 (include)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  Policy CMP0104 is not set: CMAKE_CUDA_ARCHITECTURES now detected for NVCC,\n",
            "  empty CUDA_ARCHITECTURES not allowed.  Run \"cmake --help-policy CMP0104\"\n",
            "  for policy details.  Use the cmake_policy command to set the policy and\n",
            "  suppress this warning.\n",
            "\n",
            "  CUDA_ARCHITECTURES is empty for target \"decord\".\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-13.0\n",
            "-- Found CUDA_CUDA_LIBRARY=/usr/local/cuda-13.0/lib64/stubs/libcuda.so\n",
            "-- Found CUDA_CUDART_LIBRARY=/usr/local/cuda-13.0/lib64/libcudart.so\n",
            "-- Found CUDA_NVRTC_LIBRARY=/usr/local/cuda-13.0/lib64/libnvrtc.so\n",
            "-- Found CUDA_CUDNN_LIBRARY=/usr/lib/aarch64-linux-gnu/libcudnn.so\n",
            "-- Found CUDA_CUBLAS_LIBRARY=/usr/local/cuda-13.0/lib64/libcublas.so\n",
            "-- Found CUDA_NVIDIA_ML_LIBRARY=/usr/local/cuda-13.0/lib64/stubs/libnvidia-ml.so\n",
            "-- Found CUDA_NVCUVID_LIBRARY=/usr/local/cuda-13.0/lib64/libnvcuvid.so\n",
            "-- Build with CUDA support\n",
            "-- Configuring done (1.6s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /tmp/build_decord_iy8b9ur5/decord/build\n",
            "[+] Running make…\n",
            "[  2%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/audio/audio_interface.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/cpu_device_api.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/dso_module.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/c_runtime_api.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/audio/audio_reader.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/file_util.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/str_util.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/module.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/registry.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/module_util.cc.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/ndarray.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/threading_backend.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/workspace_pool.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/system_lib_module.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/sampler/random_file_order_sampler.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/thread_pool.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/sampler/sequential_sampler.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/sampler/smart_random_sampler.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/sampler/random_sampler.cc.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/logging.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/storage_pool.cc.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/video_interface.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/video_loader.cc.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/video_reader.cc.o\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc: In member function ‘int decord::AudioReader::Decode(std::string, int)’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:131:52: warning: ‘AVCodecParameters::channels’ is deprecated [-Wdeprecated-declarations]\n",
            "  131 |                 numChannels = tempCodecParameters->channels;\n",
            "      |                                                    ^~~~~~~~\n",
            "In file included from /usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:53,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/audio/../../include/decord/../../src/video/ffmpeg/ffmpeg_common.h:23,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/audio/../../include/decord/audio_interface.h:9,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.h:10,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:5:\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/codec_par.h:166:14: note: declared here\n",
            "  166 |     int      channels;\n",
            "      |              ^~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:131:52: warning: ‘AVCodecParameters::channels’ is deprecated [-Wdeprecated-declarations]\n",
            "  131 |                 numChannels = tempCodecParameters->channels;\n",
            "      |                                                    ^~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/codec_par.h:166:14: note: declared here\n",
            "  166 |     int      channels;\n",
            "      |              ^~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:131:52: warning: ‘AVCodecParameters::channels’ is deprecated [-Wdeprecated-declarations]\n",
            "  131 |                 numChannels = tempCodecParameters->channels;\n",
            "      |                                                    ^~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/codec_par.h:166:14: note: declared here\n",
            "  166 |     int      channels;\n",
            "      |              ^~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc: In member function ‘void decord::AudioReader::HandleFrame(AVCodecContext*, AVFrame*)’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:232:99: warning: ‘AVFrame::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  232 |         int outNumChannels = av_get_channel_layout_nb_channels(mono ? AV_CH_LAYOUT_MONO : pFrame->channel_layout);\n",
            "      |                                                                                                   ^~~~~~~~~~~~~~\n",
            "In file included from /usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:36:\n",
            "/usr/include/aarch64-linux-gnu/libavutil/frame.h:575:14: note: declared here\n",
            "  575 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:232:99: warning: ‘AVFrame::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  232 |         int outNumChannels = av_get_channel_layout_nb_channels(mono ? AV_CH_LAYOUT_MONO : pFrame->channel_layout);\n",
            "      |                                                                                                   ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavutil/frame.h:575:14: note: declared here\n",
            "  575 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:232:99: warning: ‘AVFrame::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  232 |         int outNumChannels = av_get_channel_layout_nb_channels(mono ? AV_CH_LAYOUT_MONO : pFrame->channel_layout);\n",
            "      |                                                                                                   ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavutil/frame.h:575:14: note: declared here\n",
            "  575 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:232:63: warning: ‘int av_get_channel_layout_nb_channels(uint64_t)’ is deprecated [-Wdeprecated-declarations]\n",
            "  232 |         int outNumChannels = av_get_channel_layout_nb_channels(mono ? AV_CH_LAYOUT_MONO : pFrame->channel_layout);\n",
            "      |                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "In file included from /usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:34:\n",
            "/usr/include/aarch64-linux-gnu/libavutil/channel_layout.h:498:5: note: declared here\n",
            "  498 | int av_get_channel_layout_nb_channels(uint64_t channel_layout);\n",
            "      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc: In member function ‘void decord::AudioReader::InitSWR(AVCodecContext*)’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:284:28: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  284 |         if (pCodecContext->channel_layout == 0) {\n",
            "      |                            ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:284:28: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  284 |         if (pCodecContext->channel_layout == 0) {\n",
            "      |                            ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:284:28: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  284 |         if (pCodecContext->channel_layout == 0) {\n",
            "      |                            ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:28: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                            ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:28: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                            ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:28: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                            ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:91: warning: ‘AVCodecContext::channels’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                                                                                           ^~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1072:9: note: declared here\n",
            " 1072 |     int channels;\n",
            "      |         ^~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:91: warning: ‘AVCodecContext::channels’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                                                                                           ^~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1072:9: note: declared here\n",
            " 1072 |     int channels;\n",
            "      |         ^~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:91: warning: ‘AVCodecContext::channels’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                                                                                           ^~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1072:9: note: declared here\n",
            " 1072 |     int channels;\n",
            "      |         ^~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:285:74: warning: ‘int64_t av_get_default_channel_layout(int)’ is deprecated [-Wdeprecated-declarations]\n",
            "  285 |             pCodecContext->channel_layout = av_get_default_channel_layout( pCodecContext->channels );\n",
            "      |                                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavutil/channel_layout.h:506:9: note: declared here\n",
            "  506 | int64_t av_get_default_channel_layout(int nb_channels);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:287:83: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  287 |         av_opt_set_channel_layout(this->swr, \"in_channel_layout\",  pCodecContext->channel_layout, 0);\n",
            "      |                                                                                   ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:287:83: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  287 |         av_opt_set_channel_layout(this->swr, \"in_channel_layout\",  pCodecContext->channel_layout, 0);\n",
            "      |                                                                                   ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:287:83: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  287 |         av_opt_set_channel_layout(this->swr, \"in_channel_layout\",  pCodecContext->channel_layout, 0);\n",
            "      |                                                                                   ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:287:34: warning: ‘int av_opt_set_channel_layout(void*, const char*, int64_t, int)’ is deprecated [-Wdeprecated-declarations]\n",
            "  287 |         av_opt_set_channel_layout(this->swr, \"in_channel_layout\",  pCodecContext->channel_layout, 0);\n",
            "      |         ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "In file included from /tmp/build_decord_iy8b9ur5/decord/src/audio/../../include/decord/../../src/video/ffmpeg/ffmpeg_common.h:32:\n",
            "/usr/include/aarch64-linux-gnu/libavutil/opt.h:702:5: note: declared here\n",
            "  702 | int av_opt_set_channel_layout(void *obj, const char *name, int64_t ch_layout, int search_flags);\n",
            "      |     ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:288:110: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  288 |         av_opt_set_channel_layout(this->swr, \"out_channel_layout\", mono ? AV_CH_LAYOUT_MONO : pCodecContext->channel_layout,  0);\n",
            "      |                                                                                                              ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:288:110: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  288 |         av_opt_set_channel_layout(this->swr, \"out_channel_layout\", mono ? AV_CH_LAYOUT_MONO : pCodecContext->channel_layout,  0);\n",
            "      |                                                                                                              ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:288:110: warning: ‘AVCodecContext::channel_layout’ is deprecated [-Wdeprecated-declarations]\n",
            "  288 |         av_opt_set_channel_layout(this->swr, \"out_channel_layout\", mono ? AV_CH_LAYOUT_MONO : pCodecContext->channel_layout,  0);\n",
            "      |                                                                                                              ^~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavcodec/avcodec.h:1130:14: note: declared here\n",
            " 1130 |     uint64_t channel_layout;\n",
            "      |              ^~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/audio/audio_reader.cc:288:34: warning: ‘int av_opt_set_channel_layout(void*, const char*, int64_t, int)’ is deprecated [-Wdeprecated-declarations]\n",
            "  288 |         av_opt_set_channel_layout(this->swr, \"out_channel_layout\", mono ? AV_CH_LAYOUT_MONO : pCodecContext->channel_layout,  0);\n",
            "      |         ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/usr/include/aarch64-linux-gnu/libavutil/opt.h:702:5: note: declared here\n",
            "  702 | int av_opt_set_channel_layout(void *obj, const char *name, int64_t ch_layout, int search_flags);\n",
            "      |     ^~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/ffmpeg/filter_graph.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/ffmpeg/threaded_decoder.cc.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_context.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_decoder_impl.cc.o\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/build_decord_iy8b9ur5/decord/src/video/video_reader.cc: In member function ‘virtual double decord::VideoReader::GetRotation() const’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/video_reader.cc:557:53: warning: ‘uint8_t* av_stream_get_side_data(const AVStream*, AVPacketSideDataType, size_t*)’ is deprecated [-Wdeprecated-declarations]\n",
            "  557 |     uint8_t* displaymatrix = av_stream_get_side_data(active_st, AV_PKT_DATA_DISPLAYMATRIX, NULL);\n",
            "      |                              ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "In file included from /tmp/build_decord_iy8b9ur5/decord/src/video/ffmpeg/ffmpeg_common.h:25,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/video/threaded_decoder_interface.h:10,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/video/video_reader.h:10,\n",
            "                 from /tmp/build_decord_iy8b9ur5/decord/src/video/video_reader.cc:7:\n",
            "/usr/include/aarch64-linux-gnu/libavformat/avformat.h:1913:10: note: declared here\n",
            " 1913 | uint8_t *av_stream_get_side_data(const AVStream *stream,\n",
            "      |          ^~~~~~~~~~~~~~~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/ffmpeg/threaded_decoder.cc: In member function ‘void decord::ffmpeg::FFMPEGThreadedDecoder::WorkerThread()’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/ffmpeg/threaded_decoder.cc:176:26: warning: catching polymorphic type ‘struct dmlc::Error’ by value [-Wcatch-value=]\n",
            "  176 |     } catch (dmlc::Error error) {\n",
            "      |                          ^~~~~\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_mapped_frame.cc.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_parser.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_stream.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_texture.cc.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/video/nvcodec/cuda_threaded_decoder.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/cuda/cuda_device_api.cc.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/decord.dir/src/runtime/cuda/cuda_module.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object CMakeFiles/decord.dir/src/improc/improc.cu.o\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "In function ‘const char* decord::cuda::GetVideoCodecString(cudaVideoCodec)’,\n",
            "    inlined from ‘const char* decord::cuda::GetVideoCodecString(cudaVideoCodec)’ at /tmp/build_decord_iy8b9ur5/decord/src/video/nvcodec/cuda_decoder_impl.cc:15:14:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/nvcodec/cuda_decoder_impl.cc:44:37: warning: array subscript 1230591318 is above array bounds of ‘decord::cuda::GetVideoCodecString(cudaVideoCodec)::<unnamed struct> [17]’ [-Warray-bounds=]\n",
            "   44 |             return aCodecName[eCodec].name;\n",
            "      |                    ~~~~~~~~~~~~~~~~~^\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/nvcodec/cuda_decoder_impl.cc: In function ‘const char* decord::cuda::GetVideoCodecString(cudaVideoCodec)’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/nvcodec/cuda_decoder_impl.cc:19:7: note: while referencing ‘aCodecName’\n",
            "   19 |     } aCodecName [] = {\n",
            "      |       ^~~~~~~~~~\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/nvcodec/cuda_threaded_decoder.cc: In member function ‘void decord::cuda::CUThreadedDecoder::LaunchThread()’:\n",
            "/tmp/build_decord_iy8b9ur5/decord/src/video/nvcodec/cuda_threaded_decoder.cc:304:24: warning: catching polymorphic type ‘struct dmlc::Error’ by value [-Wcatch-value=]\n",
            "  304 |   } catch (dmlc::Error error) {\n",
            "      |                        ^~~~~\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100%] \u001b[1m\u001b[32mLinking CXX shared library libdecord.so\u001b[0m\n",
            "[100%] Built target decord\n",
            "[+] Installing Python bindings…\n",
            "Processing /tmp/build_decord_iy8b9ur5/decord/python\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.14.0 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from decord==0.6.0) (1.26.0)\n",
            "Building wheels for collected packages: decord\n",
            "  Building wheel for decord (pyproject.toml): started\n",
            "  Building wheel for decord (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for decord: filename=decord-0.6.0-cp311-cp311-linux_aarch64.whl size=5913590 sha256=f11ac6f1efafcf34150091dd26861b4a0beead760e26740fee809e2c08632ab6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jcxg9zg9/wheels/96/34/7b/44cdeb71fb003e75b2ab4fca0cdb99586439cb75879cb2484a\n",
            "Successfully built decord\n",
            "Installing collected packages: decord\n",
            "Successfully installed decord-0.6.0\n",
            "[✓] decord installed from: /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages/decord/__init__.py\n",
            "[✓] cpu: <function cpu at 0xfa4e6d884900> VideoReader: <class 'decord.video_reader.VideoReader'>\n",
            "[+] Cleaning temp dir: /tmp/build_decord_iy8b9ur5\n",
            "[✓] Done.\n"
          ]
        }
      ],
      "source": [
        "def install_decord_ffmpeg6():\n",
        "    \"\"\"Build & install decord 0.6.0 with CUDA + FFmpeg 6 support into CURRENT env.\n",
        "    - Builds in a temp dir (no clutter in your repo)\n",
        "    - Applies the minimal FFmpeg-6 compatibility patches\n",
        "    \"\"\"\n",
        "    import sys, subprocess, tempfile, shutil, os\n",
        "    from pathlib import Path\n",
        "\n",
        "    print(\"Using Python:\", sys.executable)\n",
        "\n",
        "    # 0) Uninstall any broken/old decord\n",
        "    print(\"[-] Uninstalling existing decord (if any)…\")\n",
        "    for _ in range(3):\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"decord\"], check=False)\n",
        "\n",
        "    # 1) Work in a temporary directory\n",
        "    tmpdir = Path(tempfile.mkdtemp(prefix=\"build_decord_\"))\n",
        "    print(\"[+] Temp dir:\", tmpdir)\n",
        "\n",
        "    print(\"[+] Cloning decord…\")\n",
        "    subprocess.run(\n",
        "        [\"git\", \"clone\", \"--recursive\", \"https://github.com/dmlc/decord.git\"],\n",
        "        cwd=tmpdir,\n",
        "        check=True,\n",
        "    )\n",
        "    decord_root = tmpdir / \"decord\"\n",
        "\n",
        "    # --- Patches for FFmpeg 6 ---\n",
        "\n",
        "    # 1) ffmpeg_common.h – add bsf.h\n",
        "    ffmpeg_common = decord_root / \"src\" / \"video\" / \"ffmpeg\" / \"ffmpeg_common.h\"\n",
        "    txt = ffmpeg_common.read_text()\n",
        "    needle = \"#include <libavcodec/avcodec.h>\\n\"\n",
        "    insert = needle + \"#include <libavcodec/bsf.h>\\n\"\n",
        "    if \"#include <libavcodec/bsf.h>\" not in txt:\n",
        "        if needle not in txt:\n",
        "            raise RuntimeError(\"avcodec.h include not found in ffmpeg_common.h\")\n",
        "        ffmpeg_common.write_text(txt.replace(needle, insert))\n",
        "\n",
        "    # 2) video_reader.cc – const AVCodec + cast in av_find_best_stream\n",
        "    video_reader = decord_root / \"src\" / \"video\" / \"video_reader.cc\"\n",
        "    vr = video_reader.read_text()\n",
        "    vr = vr.replace(\"    AVCodec *dec = nullptr;\\n\", \"    const AVCodec *dec = nullptr;\\n\")\n",
        "    vr = vr.replace(\"    AVCodec *dec = NULL;\\n\", \"    const AVCodec *dec = nullptr;\\n\")\n",
        "    if \"&dec, 0);\" in vr:\n",
        "        vr = vr.replace(\"&dec, 0);\", \"(const AVCodec**)&dec, 0);\")\n",
        "    video_reader.write_text(vr)\n",
        "\n",
        "    # 3) cuda_threaded_decoder.{h,cc} – AVInputFormat* -> const AVInputFormat*\n",
        "    cuda_h  = decord_root / \"src\" / \"video\" / \"nvcodec\" / \"cuda_threaded_decoder.h\"\n",
        "    cuda_cc = decord_root / \"src\" / \"video\" / \"nvcodec\" / \"cuda_threaded_decoder.cc\"\n",
        "\n",
        "    h = cuda_h.read_text()\n",
        "    h = h.replace(\n",
        "        \"CUThreadedDecoder(int device_id, AVCodecParameters *codecpar, AVInputFormat *iformat);\",\n",
        "        \"CUThreadedDecoder(int device_id, AVCodecParameters *codecpar, const AVInputFormat *iformat);\",\n",
        "    )\n",
        "    h = h.replace(\n",
        "        \"void InitBitStreamFilter(AVCodecParameters *codecpar, AVInputFormat *iformat);\",\n",
        "        \"void InitBitStreamFilter(AVCodecParameters *codecpar, const AVInputFormat *iformat);\",\n",
        "    )\n",
        "    cuda_h.write_text(h)\n",
        "\n",
        "    c = cuda_cc.read_text()\n",
        "    c = c.replace(\n",
        "        \"CUThreadedDecoder::CUThreadedDecoder(int device_id, AVCodecParameters *codecpar, AVInputFormat *iformat)\",\n",
        "        \"CUThreadedDecoder::CUThreadedDecoder(int device_id, AVCodecParameters *codecpar, const AVInputFormat *iformat)\",\n",
        "    )\n",
        "    c = c.replace(\n",
        "        \"void CUThreadedDecoder::InitBitStreamFilter(AVCodecParameters *codecpar, AVInputFormat *iformat)\",\n",
        "        \"void CUThreadedDecoder::InitBitStreamFilter(AVCodecParameters *codecpar, const AVInputFormat *iformat)\",\n",
        "    )\n",
        "    cuda_cc.write_text(c)\n",
        "\n",
        "    # --- Build C++ backend ---\n",
        "    build_dir = decord_root / \"build\"\n",
        "    build_dir.mkdir()\n",
        "    print(\"[+] Running CMake…\")\n",
        "    subprocess.run(\n",
        "        [\"cmake\", \"..\", \"-DUSE_CUDA=ON\", \"-DCMAKE_BUILD_TYPE=Release\"],\n",
        "        cwd=build_dir,\n",
        "        check=True,\n",
        "    )\n",
        "    print(\"[+] Running make…\")\n",
        "    subprocess.run(\n",
        "        [\"make\", f\"-j{os.cpu_count() or 4}\"],\n",
        "        cwd=build_dir,\n",
        "        check=True,\n",
        "    )\n",
        "\n",
        "    # --- Install Python package ---\n",
        "    print(\"[+] Installing Python bindings…\")\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \".\"],\n",
        "        cwd=decord_root / \"python\",\n",
        "        check=True,\n",
        "    )\n",
        "\n",
        "    # --- Verify & cleanup ---\n",
        "    import sys as _sys\n",
        "    _sys.modules.pop(\"decord\", None)\n",
        "    import decord\n",
        "    from decord import cpu, VideoReader\n",
        "\n",
        "    print(\"[✓] decord installed from:\", getattr(decord, \"__file__\", None))\n",
        "    print(\"[✓] cpu:\", cpu, \"VideoReader:\", VideoReader)\n",
        "\n",
        "    print(\"[+] Cleaning temp dir:\", tmpdir)\n",
        "    shutil.rmtree(tmpdir)\n",
        "    print(\"[✓] Done.\")\n",
        "\n",
        "\n",
        "# >>> Run the installer\n",
        "install_decord_ffmpeg6()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/valentinweyer/projects/handball-computer-vision\n",
            "fatal: destination path 'sam3' already exists and is not an empty directory.\n",
            "/home/valentinweyer/projects/handball-computer-vision/sam3\n",
            "Obtaining file:///home/valentinweyer/projects/handball-computer-vision/sam3\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: timm>=1.0.17 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (1.0.22)\n",
            "Requirement already satisfied: numpy==1.26 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (1.26.0)\n",
            "Requirement already satisfied: tqdm in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: ftfy==6.1.1 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (2025.11.3)\n",
            "Requirement already satisfied: iopath>=0.1.10 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (0.1.10)\n",
            "Requirement already satisfied: typing_extensions in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: huggingface_hub in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sam3==0.1.0) (0.36.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from ftfy==6.1.1->sam3==0.1.0) (0.2.14)\n",
            "Requirement already satisfied: portalocker in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from iopath>=0.1.10->sam3==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: torch in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from timm>=1.0.17->sam3==0.1.0) (2.9.1+cu130)\n",
            "Requirement already satisfied: torchvision in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from timm>=1.0.17->sam3==0.1.0) (0.24.1)\n",
            "Requirement already satisfied: pyyaml in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from timm>=1.0.17->sam3==0.1.0) (6.0.3)\n",
            "Requirement already satisfied: safetensors in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from timm>=1.0.17->sam3==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from huggingface_hub->sam3==0.1.0) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from huggingface_hub->sam3==0.1.0) (2025.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from huggingface_hub->sam3==0.1.0) (24.2)\n",
            "Requirement already satisfied: requests in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from huggingface_hub->sam3==0.1.0) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from huggingface_hub->sam3==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from requests->huggingface_hub->sam3==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from requests->huggingface_hub->sam3==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from requests->huggingface_hub->sam3==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from requests->huggingface_hub->sam3==0.1.0) (2025.11.12)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (13.0.48)\n",
            "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (13.0.48)\n",
            "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (13.0.48)\n",
            "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (9.13.0.50)\n",
            "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (13.0.0.19)\n",
            "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.0.0.15)\n",
            "Requirement already satisfied: nvidia-curand==10.4.0.35 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (10.4.0.35)\n",
            "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.0.3.29)\n",
            "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.6.2.49)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (2.27.7)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.3.24)\n",
            "Requirement already satisfied: nvidia-nvtx==13.0.39 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (13.0.39)\n",
            "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (13.0.39)\n",
            "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (1.15.0.42)\n",
            "Requirement already satisfied: triton==3.5.1 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from sympy>=1.13.3->torch->timm>=1.0.17->sam3==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from jinja2->torch->timm>=1.0.17->sam3==0.1.0) (3.0.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages (from torchvision->timm>=1.0.17->sam3==0.1.0) (11.3.0)\n",
            "Building wheels for collected packages: sam3\n",
            "  Building editable for sam3 (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sam3: filename=sam3-0.1.0-0.editable-py3-none-any.whl size=15232 sha256=4b2814b1c664a3dfa867528eeed684e3cb47e35edc89f9d329f43951427e06ad\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c0yn3y2l/wheels/12/21/ec/794e47b3d9c99484dc80e108804fb5126dbbebb5f20f9d2aae\n",
            "Successfully built sam3\n",
            "Installing collected packages: sam3\n",
            "  Attempting uninstall: sam3\n",
            "    Found existing installation: sam3 0.1.0\n",
            "    Uninstalling sam3-0.1.0:\n",
            "      Successfully uninstalled sam3-0.1.0\n",
            "Successfully installed sam3-0.1.0\n",
            "/home/valentinweyer/projects/handball-computer-vision\n"
          ]
        }
      ],
      "source": [
        "%cd ../\n",
        "!git clone https://github.com/facebookresearch/sam3.git\n",
        "%cd sam3\n",
        "!pip install -e .\n",
        "%cd ../\n",
        "!rm -rf sam3/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FLe6WypmpRuR"
      },
      "outputs": [],
      "source": [
        "!pip install -q supervision jupyter_bbox_widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_z7Y5R38pX3p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/valentinweyer/miniforge3/envs/handball-computer-vision/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
            "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
            "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
            "    (8.0) - (12.0)\n",
            "    \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use SAM3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "059de4296940416aa97ebf2d044e68df",
            "9e0e346e5c8a4016bb68c45402895583",
            "818a874cd76e415691612f800fd66f44",
            "600f104c39164b05b48ddc8fa19ffd02",
            "efee571f74f54aa98e850dbd8ed78040",
            "5e41cd9ff9ec4a25a4323deabe293948",
            "160e0ba36eb6442f9533002d5bed58e5",
            "8b3758798d7a4713a73ccdf6959810a8",
            "54e72cc8e91a4820b3f40c186b3b76c4",
            "6d6c374602f74a13b6a6084d5886beb3",
            "3300addf73a34754b440bb2fc1b75f60",
            "522ec4c2fcc84351ad4b3044e14035f1",
            "34813a61425547c0821fcd78e8b63280",
            "42a5911db17b4b97923c5924ede4489d",
            "fd07ce939e164af3a756cff6974b4fcc",
            "33d9320beb8943dd83814a4ae36452ec",
            "c925e0b6c12e43d08f009d986aa05d44",
            "88de41b6bd28466095f8f991d9da8548",
            "3d6eaae80610454a925aeebd3783e9f3",
            "dcb80c052d104e2580e0ccb42cf9361b",
            "90c31cd82bc543d294f18b1183cbd104",
            "1d71a5330e64440090b204dc03068024"
          ]
        },
        "id": "9a4gV9htpXie",
        "outputId": "22792f56-5b4f-475b-f7c4-e7200addb472"
      },
      "outputs": [],
      "source": [
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "from sam3.model_builder import build_sam3_image_model\n",
        "\n",
        "\n",
        "model = build_sam3_image_model()\n",
        "processor = Sam3Processor(model, confidence_threshold=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKb0BRmopYfs"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "def from_sam(sam_result: dict) -> sv.Detections:\n",
        "    xyxy = sam_result[\"boxes\"].to(torch.float32).cpu().numpy()\n",
        "    confidence = sam_result[\"scores\"].to(torch.float32).cpu().numpy()\n",
        "\n",
        "    mask = sam_result[\"masks\"].to(torch.bool)\n",
        "    mask = mask.reshape(mask.shape[0], mask.shape[2], mask.shape[3]).cpu().numpy()\n",
        "\n",
        "    return sv.Detections(\n",
        "        xyxy=xyxy,\n",
        "        confidence=confidence,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "from PIL import Image\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "COLOR = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
        "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "\n",
        "def annotate(image: Image.Image, detections: sv.Detections, label: Optional[str] = None) -> Image.Image:\n",
        "    text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
        "\n",
        "    mask_annotator = sv.MaskAnnotator(\n",
        "        color=COLOR,\n",
        "        color_lookup=sv.ColorLookup.INDEX,\n",
        "        opacity=0.6\n",
        "    )\n",
        "    box_annotator = sv.BoxAnnotator(\n",
        "        color=COLOR,\n",
        "        color_lookup=sv.ColorLookup.INDEX,\n",
        "        thickness=1\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=COLOR,\n",
        "        color_lookup=sv.ColorLookup.INDEX,\n",
        "        text_scale=0.4,\n",
        "        text_padding=5,\n",
        "        text_color=sv.Color.BLACK,\n",
        "        text_thickness=1\n",
        "    )\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    annotated_image = mask_annotator.annotate(annotated_image, detections)\n",
        "    annotated_image = box_annotator.annotate(annotated_image, detections)\n",
        "\n",
        "    if label:\n",
        "        labels = [\n",
        "            f\"{label} {confidence:.2f}\"\n",
        "            for confidence in detections.confidence\n",
        "        ]\n",
        "        annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
        "\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6037e77"
      },
      "source": [
        "\n",
        "Install the `roboflow` library using pip as requested to enable dataset downloading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2da034c",
        "outputId": "35961de1-0730-49da-ca22-94b2ff206424"
      },
      "outputs": [],
      "source": [
        "!pip install -q roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0f66fb2"
      },
      "source": [
        "## Download Roboflow Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bf707b",
        "outputId": "d251261c-93d3-4f60-ae2b-93da5cfa0bc8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=roboflow_key)\n",
        "\n",
        "# Access workspace and project\n",
        "project = rf.workspace(\"valentin-weyer-xasiu\").project(\"player-and-handball-detection-3z9xf\")\n",
        "\n",
        "# Download dataset\n",
        "dataset = project.version(1).download(\"yolov8\")\n",
        "\n",
        "print(\"Dataset downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ff9481f"
      },
      "source": [
        "## Run SAM3 Inference and Visualize\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f671718b",
        "outputId": "38c9fa33-62b9-461b-979a-6b8de61f52cb"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Define path to validation images (adjust if needed based on dataset structure)\n",
        "IMAGE_DIR = os.path.join(dataset.location, \"valid\", \"images\")\n",
        "\n",
        "# Get all jpg images\n",
        "image_paths = glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\"))\n",
        "\n",
        "# Select a random sample if images exist\n",
        "if len(image_paths) > 0:\n",
        "    sample_paths = random.sample(image_paths, min(3, len(image_paths)))\n",
        "\n",
        "    PROMPT = \"jersey number\"\n",
        "    print(f\"Running inference on {len(sample_paths)} images with prompt: '{PROMPT}'\\n\")\n",
        "\n",
        "    for img_path in sample_paths:\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Run SAM3 Inference\n",
        "        inference_state = processor.set_image(image)\n",
        "        inference_state = processor.set_text_prompt(state=inference_state, prompt=PROMPT)\n",
        "\n",
        "        # Process results\n",
        "        detections = from_sam(sam_result=inference_state)\n",
        "        detections = detections[detections.confidence > 0.4]\n",
        "\n",
        "        # Visualize\n",
        "        print(f\"Image: {os.path.basename(img_path)}\")\n",
        "        print(f\"Found {len(detections)} detections.\")\n",
        "        annotated_image = annotate(image, detections, label=PROMPT)\n",
        "        display(annotated_image)\n",
        "        print(\"-\" * 50)\n",
        "else:\n",
        "    print(f\"No images found in {IMAGE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb628a21"
      },
      "source": [
        "# Task\n",
        "Parse the `data.yaml` file from the downloaded dataset (located at `dataset.location`) to retrieve class names. Iterate through the train, valid, and test image sets to convert existing YOLO labels to COCO format. For each image, also run SAM3 inference to detect \"jersey number\", assigning these detections a new category ID. Consolidate all annotations into a single `_annotations.coco.json` file. Upload this file to the Roboflow project to update annotations, trigger the generation of dataset Version 2 via the API, and summarize the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4966f63"
      },
      "source": [
        "## Load Dataset Metadata\n",
        "\n",
        "### Subtask:\n",
        "Parse the `data.yaml` file to extract existing class names and define the ID for the new class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed6c293e",
        "outputId": "25a9c3d7-ecf5-4d30-d79c-fb5d11eea396"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Construct path to data.yaml\n",
        "yaml_path = os.path.join(dataset.location, \"data.yaml\")\n",
        "\n",
        "# Load the yaml file\n",
        "with open(yaml_path, 'r') as f:\n",
        "    data_yaml = yaml.safe_load(f)\n",
        "\n",
        "# Extract class names\n",
        "# YOLO data.yaml 'names' is typically a list of strings or a dictionary.\n",
        "raw_names = data_yaml.get('names', [])\n",
        "\n",
        "# Create a dictionary mapping ID to class name\n",
        "if isinstance(raw_names, dict):\n",
        "    existing_classes = {int(k): v for k, v in raw_names.items()}\n",
        "else:\n",
        "    existing_classes = {i: name for i, name in enumerate(raw_names)}\n",
        "\n",
        "# Define new class details\n",
        "new_class_name = \"jersey number\"\n",
        "new_class_id = len(existing_classes)\n",
        "\n",
        "print(\"Existing classes:\", existing_classes)\n",
        "print(f\"New class '{new_class_name}' will be assigned ID: {new_class_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49653317"
      },
      "source": [
        "## Generate Consolidated COCO JSON\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the dataset, converting existing YOLO labels and generating new SAM3 predictions into a single COCO JSON file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "629fc550"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "import os   \n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Initialize COCO structure\n",
        "coco_dataset = {\n",
        "    \"info\": {\n",
        "        \"description\": \"Merged Dataset with SAM3 Predictions\",\n",
        "        \"url\": \"\",\n",
        "        \"version\": \"1.0\",\n",
        "        \"year\": 2025,\n",
        "        \"contributor\": \"Agent\",\n",
        "        \"date_created\": \"2025-11-21\"\n",
        "    },\n",
        "    \"licenses\": [],\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Add existing categories\n",
        "for class_id, class_name in existing_classes.items():\n",
        "    coco_dataset[\"categories\"].append({\n",
        "        \"id\": int(class_id),\n",
        "        \"name\": class_name,\n",
        "        \"supercategory\": \"none\"\n",
        "    })\n",
        "\n",
        "# Add the new category\n",
        "coco_dataset[\"categories\"].append({\n",
        "    \"id\": new_class_id,\n",
        "    \"name\": new_class_name,\n",
        "    \"supercategory\": \"none\"\n",
        "})\n",
        "\n",
        "# Helper for YOLO to COCO conversion\n",
        "def yolo_to_coco(x_center, y_center, w, h, img_w, img_h):\n",
        "    w_pixel = w * img_w\n",
        "    h_pixel = h * img_h\n",
        "    x_min = (x_center * img_w) - (w_pixel / 2)\n",
        "    y_min = (y_center * img_h) - (h_pixel / 2)\n",
        "    return [x_min, y_min, w_pixel, h_pixel]\n",
        "\n",
        "annotation_id = 0\n",
        "image_id_counter = 0\n",
        "\n",
        "splits = ['train', 'valid', 'test']\n",
        "\n",
        "print(\"Starting dataset processing...\")\n",
        "\n",
        "for split in splits:\n",
        "    image_dir = os.path.join(dataset.location, split, \"images\")\n",
        "    label_dir = os.path.join(dataset.location, split, \"labels\")\n",
        "\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"Skipping {split} (directory not found)\")\n",
        "        continue\n",
        "\n",
        "    image_files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
        "    print(f\"Processing {len(image_files)} images in {split}...\")\n",
        "\n",
        "    for img_path in tqdm(image_files):\n",
        "        filename = os.path.basename(img_path)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            img_w, img_h = image.size\n",
        "\n",
        "            # Add image entry\n",
        "            image_entry = {\n",
        "                \"id\": image_id_counter,\n",
        "                \"file_name\": filename,\n",
        "                \"width\": img_w,\n",
        "                \"height\": img_h,\n",
        "                \"license\": None,\n",
        "                \"date_captured\": None\n",
        "            }\n",
        "            coco_dataset[\"images\"].append(image_entry)\n",
        "\n",
        "            # 1. Process Existing YOLO Labels\n",
        "            label_path = os.path.join(label_dir, filename.replace(\".jpg\", \".txt\"))\n",
        "            if os.path.exists(label_path):\n",
        "                with open(label_path, \"r\") as f:\n",
        "                    lines = f.readlines()\n",
        "                    for line in lines:\n",
        "                        parts = line.strip().split()\n",
        "                        cls_id = int(parts[0])\n",
        "                        # YOLO format: class x_center y_center width height\n",
        "                        bbox = yolo_to_coco(\n",
        "                            float(parts[1]), float(parts[2]),\n",
        "                            float(parts[3]), float(parts[4]),\n",
        "                            img_w, img_h\n",
        "                        )\n",
        "\n",
        "                        annotation = {\n",
        "                            \"id\": annotation_id,\n",
        "                            \"image_id\": image_id_counter,\n",
        "                            \"category_id\": cls_id,\n",
        "                            \"bbox\": bbox,\n",
        "                            \"area\": bbox[2] * bbox[3],\n",
        "                            \"segmentation\": [],\n",
        "                            \"iscrowd\": 0\n",
        "                        }\n",
        "                        coco_dataset[\"annotations\"].append(annotation)\n",
        "                        annotation_id += 1\n",
        "\n",
        "            # 2. Process SAM3 Predictions (New Class)\n",
        "            inference_state = processor.set_image(image)\n",
        "            inference_state = processor.set_text_prompt(state=inference_state, prompt=new_class_name)\n",
        "            sam_results = from_sam(sam_result=inference_state)\n",
        "\n",
        "            # Filter by confidence\n",
        "            mask = sam_results.confidence > 0.3\n",
        "            filtered_detections = sam_results[mask]\n",
        "\n",
        "            for i, xyxy in enumerate(filtered_detections.xyxy):\n",
        "                x_min = float(xyxy[0])\n",
        "                y_min = float(xyxy[1])\n",
        "                w_box = float(xyxy[2] - xyxy[0])\n",
        "                h_box = float(xyxy[3] - xyxy[1])\n",
        "                bbox = [x_min, y_min, w_box, h_box]\n",
        "\n",
        "                annotation = {\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": image_id_counter,\n",
        "                    \"category_id\": new_class_id,\n",
        "                    \"bbox\": bbox,\n",
        "                    \"area\": w_box * h_box,\n",
        "                    \"segmentation\": [],\n",
        "                    \"iscrowd\": 0,\n",
        "                    \"score\": float(filtered_detections.confidence[i])\n",
        "                }\n",
        "                coco_dataset[\"annotations\"].append(annotation)\n",
        "                annotation_id += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        image_id_counter += 1\n",
        "\n",
        "output_path = \"_annotations.coco.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(coco_dataset, f)\n",
        "\n",
        "print(f\"\\nSuccessfully created {output_path}\")\n",
        "print(f\"Total Images: {len(coco_dataset['images'])}\")\n",
        "print(f\"Total Annotations: {len(coco_dataset['annotations'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import yaml\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIDENCE_THRESHOLD = 0.4\n",
        "SCOREBOARD_CONFIDENCE = 0.5\n",
        "NMS_THRESHOLD = 0.25\n",
        "# ---------------------\n",
        "\n",
        "# Ensure we have the class names\n",
        "yaml_path = os.path.join(dataset.location, \"data.yaml\")\n",
        "with open(yaml_path, 'r') as f:\n",
        "    data_yaml = yaml.safe_load(f)\n",
        "\n",
        "raw_names = data_yaml.get('names', [])\n",
        "if isinstance(raw_names, dict):\n",
        "    existing_classes = {int(k): v for k, v in raw_names.items()}\n",
        "else:\n",
        "    existing_classes = {i: name for i, name in enumerate(raw_names)}\n",
        "\n",
        "# Identify Referee Class ID automatically\n",
        "referee_ids = [k for k, v in existing_classes.items() if 'referee' in v.lower()]\n",
        "referee_id = referee_ids[0] if referee_ids else -1\n",
        "if referee_id != -1:\n",
        "    print(f\"Referee filtering enabled for Class ID: {referee_id} ({existing_classes[referee_id]})\")\n",
        "else:\n",
        "    print(\"Warning: 'Referee' class not found. Referee filtering will be skipped.\")\n",
        "\n",
        "new_class_name = \"jersey number\"\n",
        "new_class_id = len(existing_classes)\n",
        "\n",
        "# Initialize COCO structure\n",
        "coco_dataset = {\n",
        "    \"info\": {\n",
        "        \"description\": \"Dataset with Scoreboard and Referee Filtering\",\n",
        "        \"version\": \"1.4\",\n",
        "        \"year\": 2025,\n",
        "        \"contributor\": \"Agent\",\n",
        "        \"date_created\": \"2025-11-21\"\n",
        "    },\n",
        "    \"licenses\": [],\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Add categories\n",
        "for class_id, class_name in existing_classes.items():\n",
        "    coco_dataset[\"categories\"].append({\n",
        "        \"id\": int(class_id),\n",
        "        \"name\": class_name,\n",
        "        \"supercategory\": \"none\"\n",
        "    })\n",
        "coco_dataset[\"categories\"].append({\n",
        "        \"id\": new_class_id,\n",
        "        \"name\": new_class_name,\n",
        "        \"supercategory\": \"none\"\n",
        "    })\n",
        "\n",
        "def yolo_to_coco(x_center, y_center, w, h, img_w, img_h):\n",
        "    w_pixel = w * img_w\n",
        "    h_pixel = h * img_h\n",
        "    x_min = (x_center * img_w) - (w_pixel / 2)\n",
        "    y_min = (y_center * img_h) - (h_pixel / 2)\n",
        "    return [x_min, y_min, w_pixel, h_pixel]\n",
        "\n",
        "def is_center_inside_any(bbox, target_boxes):\n",
        "    \"\"\"\n",
        "    Checks if the center of the bbox is inside any of the target boxes.\n",
        "    bbox: [x1, y1, x2, y2]\n",
        "    target_boxes: list of [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    center_x = (x1 + x2) / 2\n",
        "    center_y = (y1 + y2) / 2\n",
        "    \n",
        "    for t_box in target_boxes:\n",
        "        tx1, ty1, tx2, ty2 = t_box\n",
        "        if tx1 <= center_x <= tx2 and ty1 <= center_y <= ty2:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "annotation_id = 0\n",
        "image_id_counter = 0\n",
        "splits = ['train', 'valid', 'test']\n",
        "\n",
        "print(\"Starting full dataset processing with DYNAMIC Scoreboard & REFEREE filtering...\")\n",
        "\n",
        "for split in splits:\n",
        "    image_dir = os.path.join(dataset.location, split, \"images\")\n",
        "    label_dir = os.path.join(dataset.location, split, \"labels\")\n",
        "    \n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"Skipping {split}: directory not found.\")\n",
        "        continue\n",
        "        \n",
        "    image_files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
        "    print(f\"Processing {len(image_files)} images in {split}...\")\n",
        "    \n",
        "    for img_path in tqdm(image_files):\n",
        "        filename = os.path.basename(img_path)\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            img_w, img_h = image.size\n",
        "            \n",
        "            # Image entry\n",
        "            coco_dataset[\"images\"].append({\n",
        "                \"id\": image_id_counter,\n",
        "                \"file_name\": filename,\n",
        "                \"width\": img_w,\n",
        "                \"height\": img_h,\n",
        "                \"license\": None,\n",
        "                \"date_captured\": None\n",
        "            })\n",
        "            \n",
        "            referee_boxes = []  # Store referee boxes for this image\n",
        "\n",
        "            # 1. Ground Truth (YOLO)\n",
        "            label_path = os.path.join(label_dir, filename.replace(\".jpg\", \".txt\"))\n",
        "            if os.path.exists(label_path):\n",
        "                with open(label_path, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) >= 5:\n",
        "                            cls_id = int(parts[0])\n",
        "                            bbox = yolo_to_coco(float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4]), img_w, img_h)\n",
        "                            \n",
        "                            # Add to dataset\n",
        "                            coco_dataset[\"annotations\"].append({\n",
        "                                \"id\": annotation_id,\n",
        "                                \"image_id\": image_id_counter,\n",
        "                                \"category_id\": cls_id,\n",
        "                                \"bbox\": bbox,\n",
        "                                \"area\": bbox[2] * bbox[3],\n",
        "                                \"segmentation\": [],\n",
        "                                \"iscrowd\": 0\n",
        "                            })\n",
        "                            annotation_id += 1\n",
        "                            \n",
        "                            # Collect Referee Boxes for filtering\n",
        "                            if cls_id == referee_id:\n",
        "                                # Convert COCO [x, y, w, h] to [x1, y1, x2, y2]\n",
        "                                rx1, ry1 = bbox[0], bbox[1]\n",
        "                                rx2, ry2 = rx1 + bbox[2], ry1 + bbox[3]\n",
        "                                referee_boxes.append([rx1, ry1, rx2, ry2])\n",
        "            \n",
        "            # 2. Set Image for Inference\n",
        "            inference_state = processor.set_image(image)\n",
        "            \n",
        "            # 3. Detect Scoreboard (Dynamic Filter)\n",
        "            inference_state = processor.set_text_prompt(state=inference_state, prompt=\"scoreboard\")\n",
        "            sb_results = from_sam(sam_result=inference_state)\n",
        "            \n",
        "            sb_boxes = []\n",
        "            if len(sb_results.xyxy) > 0:\n",
        "                sb_mask = sb_results.confidence > SCOREBOARD_CONFIDENCE\n",
        "                sb_detections = sb_results[sb_mask]\n",
        "                sb_boxes = sb_detections.xyxy.tolist()\n",
        "\n",
        "            # 4. Detect Jersey Numbers\n",
        "            inference_state = processor.set_text_prompt(state=inference_state, prompt=new_class_name)\n",
        "            jn_results = from_sam(sam_result=inference_state)\n",
        "            \n",
        "            # Filter by confidence\n",
        "            mask = jn_results.confidence > CONFIDENCE_THRESHOLD\n",
        "            filtered = jn_results[mask]\n",
        "            \n",
        "            # Filter 1: NMS (Overlap Removal)\n",
        "            if len(filtered.xyxy) > 0:\n",
        "                boxes_t = torch.from_numpy(filtered.xyxy).float()\n",
        "                scores_t = torch.from_numpy(filtered.confidence).float()\n",
        "                keep_indices = torchvision.ops.nms(boxes_t, scores_t, iou_threshold=NMS_THRESHOLD)\n",
        "                filtered = filtered[keep_indices.numpy()]\n",
        "            \n",
        "            # Add filtered predictions to COCO\n",
        "            for i, xyxy in enumerate(filtered.xyxy):\n",
        "                x1, y1, x2, y2 = float(xyxy[0]), float(xyxy[1]), float(xyxy[2]), float(xyxy[3])\n",
        "                w_box = x2 - x1\n",
        "                h_box = y2 - y1\n",
        "                bbox_xyxy = [x1, y1, x2, y2]\n",
        "                \n",
        "                # Filter 2: Dynamic Scoreboard Check\n",
        "                if is_center_inside_any(bbox_xyxy, sb_boxes):\n",
        "                    continue\n",
        "                \n",
        "                # Filter 3: Referee Overlap Check\n",
        "                if is_center_inside_any(bbox_xyxy, referee_boxes):\n",
        "                    continue\n",
        "                \n",
        "                # Add to annotations\n",
        "                bbox_coco = [x1, y1, w_box, h_box]\n",
        "                coco_dataset[\"annotations\"].append({\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": image_id_counter,\n",
        "                    \"category_id\": new_class_id,\n",
        "                    \"bbox\": bbox_coco,\n",
        "                    \"area\": w_box * h_box,\n",
        "                    \"segmentation\": [],\n",
        "                    \"iscrowd\": 0,\n",
        "                    \"score\": float(filtered.confidence[i])\n",
        "                })\n",
        "                annotation_id += 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            \n",
        "        image_id_counter += 1\n",
        "\n",
        "output_path = \"_annotations.coco.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(coco_dataset, f)\n",
        "\n",
        "print(f\"\\nSuccessfully created {output_path}\")\n",
        "print(f\"Total Images: {len(coco_dataset['images'])}\")\n",
        "print(f\"Total Annotations: {len(coco_dataset['annotations'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "352607fb"
      },
      "source": [
        "## Visualize Existing COCO Annotations\n",
        "\n",
        "### Subtask:\n",
        "Load the generated COCO annotations file and visualize a random sample of images with bounding boxes to verify the merged labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db45ad39"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the COCO annotations JSON file, parse it to extract image paths and annotations, and then visualize a few random samples using the supervision library to verify the correctness of the merged dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6b2d471e",
        "outputId": "6755a2d5-29f8-4e4e-fbc5-1015bad2c040"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# Path to the consolidated annotations file\n",
        "annotation_file = \"_annotations.coco.json\"\n",
        "\n",
        "if os.path.exists(annotation_file):\n",
        "    # Load the JSON\n",
        "    with open(annotation_file, \"r\") as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    images = coco_data[\"images\"]\n",
        "    annotations = coco_data[\"annotations\"]\n",
        "    categories = coco_data[\"categories\"]\n",
        "\n",
        "    print(f\"Loaded JSON with {len(images)} images, {len(annotations)} annotations, and {len(categories)} categories.\")\n",
        "\n",
        "    # Create category ID to Name mapping\n",
        "    category_map = {cat[\"id\"]: cat[\"name\"] for cat in categories}\n",
        "    print(\"Category Mapping:\", category_map)\n",
        "\n",
        "    # Select random images\n",
        "    selected_images = random.sample(images, min(10, len(images)))\n",
        "\n",
        "    for img_entry in selected_images:\n",
        "        file_name = img_entry[\"file_name\"]\n",
        "        image_id = img_entry[\"id\"]\n",
        "\n",
        "        # Find the image path\n",
        "        found_path = None\n",
        "        for split in [\"train\", \"valid\", \"test\"]:\n",
        "            potential_path = os.path.join(dataset.location, split, \"images\", file_name)\n",
        "            if os.path.exists(potential_path):\n",
        "                found_path = potential_path\n",
        "                break\n",
        "\n",
        "        if found_path:\n",
        "            # Open image\n",
        "            image = Image.open(found_path).convert(\"RGB\")\n",
        "\n",
        "            # Filter annotations for this image\n",
        "            img_anns = [ann for ann in annotations if ann[\"image_id\"] == image_id]\n",
        "\n",
        "            if not img_anns:\n",
        "                print(f\"No annotations found for {file_name}\")\n",
        "                display(image)\n",
        "                print(\"-\" * 50)\n",
        "                continue\n",
        "\n",
        "            # Prepare data for detections\n",
        "            boxes = []\n",
        "            class_ids = []\n",
        "            scores = []\n",
        "\n",
        "            for ann in img_anns:\n",
        "                x, y, w, h = ann[\"bbox\"]\n",
        "                # Convert xywh to xyxy\n",
        "                boxes.append([x, y, x + w, y + h])\n",
        "                class_ids.append(ann[\"category_id\"])\n",
        "                scores.append(ann.get(\"score\", 1.0))\n",
        "\n",
        "            # Create Detections object\n",
        "            detections = sv.Detections(\n",
        "                xyxy=np.array(boxes),\n",
        "                class_id=np.array(class_ids),\n",
        "                confidence=np.array(scores)\n",
        "            )\n",
        "\n",
        "            # Annotators\n",
        "            box_annotator = sv.BoxAnnotator()\n",
        "            label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "            # Generate labels\n",
        "            labels = [\n",
        "                f\"{category_map[class_id]} {confidence:.2f}\"\n",
        "                for class_id, confidence in zip(class_ids, scores)\n",
        "            ]\n",
        "\n",
        "            # Annotate and display\n",
        "            annotated_image = image.copy()\n",
        "            annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "            annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "            print(f\"Visualizing: {file_name}\")\n",
        "            display(annotated_image)\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "            print(f\"Image file not found: {file_name}\")\n",
        "else:\n",
        "    print(f\"Annotation file {annotation_file} not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload annotations to roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the COCO JSON we just created\n",
        "annotation_file = \"_annotations.coco.json\"\n",
        "with open(annotation_file, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Map categories: ID -> Name\n",
        "category_map = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n",
        "\n",
        "# Map Image ID -> Annotations List\n",
        "image_ann_map = {}\n",
        "for ann in coco_data[\"annotations\"]:\n",
        "    img_id = ann[\"image_id\"]\n",
        "    if img_id not in image_ann_map:\n",
        "        image_ann_map[img_id] = []\n",
        "    image_ann_map[img_id].append(ann)\n",
        "\n",
        "# Map Filename -> Image Info\n",
        "filename_to_img = {img[\"file_name\"]: img for img in coco_data[\"images\"]}\n",
        "\n",
        "def create_pascal_voc_xml(filename, width, height, annotations, output_path):\n",
        "    xml_content = []\n",
        "    xml_content.append(\"<annotation>\")\n",
        "    xml_content.append(f\"    <folder></folder>\")\n",
        "    xml_content.append(f\"    <filename>{filename}</filename>\")\n",
        "    xml_content.append(\"    <size>\")\n",
        "    xml_content.append(f\"        <width>{width}</width>\")\n",
        "    xml_content.append(f\"        <height>{height}</height>\")\n",
        "    xml_content.append(\"        <depth>3</depth>\")\n",
        "    xml_content.append(\"    </size>\")\n",
        "\n",
        "    for ann in annotations:\n",
        "        cat_id = ann[\"category_id\"]\n",
        "        name = category_map.get(cat_id, \"unknown\")\n",
        "        \n",
        "        # COCO bbox is [x_min, y_min, width, height]\n",
        "        bbox = ann[\"bbox\"]\n",
        "        xmin = bbox[0]\n",
        "        ymin = bbox[1]\n",
        "        xmax = bbox[0] + bbox[2]\n",
        "        ymax = bbox[1] + bbox[3]\n",
        "\n",
        "        xml_content.append(\"    <object>\")\n",
        "        xml_content.append(f\"        <name>{name}</name>\")\n",
        "        xml_content.append(\"        <bndbox>\")\n",
        "        xml_content.append(f\"            <xmin>{xmin}</xmin>\")\n",
        "        xml_content.append(f\"            <ymin>{ymin}</ymin>\")\n",
        "        xml_content.append(f\"            <xmax>{xmax}</xmax>\")\n",
        "        xml_content.append(f\"            <ymax>{ymax}</ymax>\")\n",
        "        xml_content.append(\"        </bndbox>\")\n",
        "        xml_content.append(\"    </object>\")\n",
        "\n",
        "    xml_content.append(\"</annotation>\")\n",
        "    \n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(xml_content))\n",
        "\n",
        "print(\"Starting upload process... This may take a while as we iterate through images.\")\n",
        "\n",
        "splits = ['train', 'valid', 'test']\n",
        "\n",
        "for split in splits:\n",
        "    image_dir = os.path.join(dataset.location, split, \"images\")\n",
        "    if not os.path.exists(image_dir):\n",
        "        continue\n",
        "\n",
        "    image_files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
        "    print(f\"Uploading {len(image_files)} images from {split}...\")\n",
        "\n",
        "    for img_path in tqdm(image_files):\n",
        "        filename = os.path.basename(img_path)\n",
        "        \n",
        "        # Get image info from COCO map\n",
        "        img_info = filename_to_img.get(filename)\n",
        "        if not img_info:\n",
        "            continue\n",
        "            \n",
        "        img_id = img_info[\"id\"]\n",
        "        anns = image_ann_map.get(img_id, [])\n",
        "        \n",
        "        # Create temporary XML annotation file\n",
        "        xml_path = img_path.replace(\".jpg\", \".xml\")\n",
        "        create_pascal_voc_xml(filename, img_info[\"width\"], img_info[\"height\"], anns, xml_path)\n",
        "        \n",
        "        try:\n",
        "            # Upload Image + XML to Roboflow\n",
        "            # This updates the existing image with new annotations\n",
        "            project.upload(image_path=img_path, annotation_path=xml_path, split=split, num_retry_uploads=3, batch_name=\"SAM3_Inference_filtered\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to upload {filename}: {e}\")\n",
        "        finally:\n",
        "            # Cleanup temp file\n",
        "            if os.path.exists(xml_path):\n",
        "                os.remove(xml_path)\n",
        "\n",
        "print(\"Upload complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "handball-computer-vision",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "059de4296940416aa97ebf2d044e68df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e0e346e5c8a4016bb68c45402895583",
              "IPY_MODEL_818a874cd76e415691612f800fd66f44",
              "IPY_MODEL_600f104c39164b05b48ddc8fa19ffd02"
            ],
            "layout": "IPY_MODEL_efee571f74f54aa98e850dbd8ed78040"
          }
        },
        "160e0ba36eb6442f9533002d5bed58e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d71a5330e64440090b204dc03068024": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3300addf73a34754b440bb2fc1b75f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33d9320beb8943dd83814a4ae36452ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34813a61425547c0821fcd78e8b63280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c925e0b6c12e43d08f009d986aa05d44",
            "placeholder": "​",
            "style": "IPY_MODEL_88de41b6bd28466095f8f991d9da8548",
            "value": "sam3.pt: 100%"
          }
        },
        "3d6eaae80610454a925aeebd3783e9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a5911db17b4b97923c5924ede4489d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d6eaae80610454a925aeebd3783e9f3",
            "max": 3450062241,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcb80c052d104e2580e0ccb42cf9361b",
            "value": 3450062241
          }
        },
        "522ec4c2fcc84351ad4b3044e14035f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34813a61425547c0821fcd78e8b63280",
              "IPY_MODEL_42a5911db17b4b97923c5924ede4489d",
              "IPY_MODEL_fd07ce939e164af3a756cff6974b4fcc"
            ],
            "layout": "IPY_MODEL_33d9320beb8943dd83814a4ae36452ec"
          }
        },
        "54e72cc8e91a4820b3f40c186b3b76c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e41cd9ff9ec4a25a4323deabe293948": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "600f104c39164b05b48ddc8fa19ffd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d6c374602f74a13b6a6084d5886beb3",
            "placeholder": "​",
            "style": "IPY_MODEL_3300addf73a34754b440bb2fc1b75f60",
            "value": " 25.8k/25.8k [00:00&lt;00:00, 2.44MB/s]"
          }
        },
        "6d6c374602f74a13b6a6084d5886beb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818a874cd76e415691612f800fd66f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3758798d7a4713a73ccdf6959810a8",
            "max": 25843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54e72cc8e91a4820b3f40c186b3b76c4",
            "value": 25843
          }
        },
        "88de41b6bd28466095f8f991d9da8548": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b3758798d7a4713a73ccdf6959810a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c31cd82bc543d294f18b1183cbd104": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e0e346e5c8a4016bb68c45402895583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e41cd9ff9ec4a25a4323deabe293948",
            "placeholder": "​",
            "style": "IPY_MODEL_160e0ba36eb6442f9533002d5bed58e5",
            "value": "config.json: 100%"
          }
        },
        "c925e0b6c12e43d08f009d986aa05d44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcb80c052d104e2580e0ccb42cf9361b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efee571f74f54aa98e850dbd8ed78040": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd07ce939e164af3a756cff6974b4fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90c31cd82bc543d294f18b1183cbd104",
            "placeholder": "​",
            "style": "IPY_MODEL_1d71a5330e64440090b204dc03068024",
            "value": " 3.45G/3.45G [00:37&lt;00:00, 265MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
