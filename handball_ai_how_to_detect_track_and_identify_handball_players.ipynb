{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS25QDv1a8_W"
   },
   "source": [
    "\n",
    "# Handball AI: How to Detect, Track, and Identify Handball Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O5hUmxdbp0e"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8KpVPsbxz1"
   },
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To run this notebook, you need to provide your HuggingFace Token and Roboflow API key.  \n",
    "\n",
    "- The `ROBOFLOW_API_KEY` is required to pull the fine-tuned RF-DETR player detector and the SmolVLM2 number recognizer from Roboflow Universe.  \n",
    "- The `HF_TOKEN` is required to pull the pretrained SigLIP model from HuggingFace.  \n",
    "\n",
    "Follow these steps:  \n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate a new token.  \n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.  \n",
    "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).  \n",
    "    - Store the HuggingFace Access Token under the name `HF_TOKEN`.  \n",
    "    - Store the Roboflow API Key under the name `ROBOFLOW_API_KEY`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "roboflow_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "huggingface_key = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = roboflow_key\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = huggingface_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfA2MikrcFWL"
   },
   "source": [
    "### Check GPU availability\n",
    "\n",
    "Let's make sure we have access to a GPU. Run the `nvidia-smi` command to verify. If you run into issues, go to `Runtime` -> `Change runtime type`, select `T4 GPU` or `L4 GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkeEI66XcVgr"
   },
   "source": [
    "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HOME = Path.cwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJT-DpbFt6Ns"
   },
   "source": [
    "## Install SAM2 real-time\n",
    "\n",
    "We will use `segment-anything-2-real-time`, an open-source fork of Metaâ€™s Segment Anything Model 2 optimized for real-time inference. After installing the repository, we will also download the required checkpoint files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, site, pathlib\n",
    "\n",
    "# 1) Make sure CUDA is visible *inside this notebook process*\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-13.0\"\n",
    "os.environ[\"PATH\"] = os.environ[\"CUDA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.environ[\"CUDA_HOME\"] + \"/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "print(\"CUDA_HOME:\", os.environ[\"CUDA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Clone repo into a fixed location (if not already there)\n",
    "base = \"/home/valentinweyer/projects/handball-computer-vision\"\n",
    "os.makedirs(base, exist_ok=True)\n",
    "os.chdir(base)\n",
    "\n",
    "if not pathlib.Path(\"segment-anything-2-real-time\").exists():\n",
    "    !git clone https://github.com/Gy920/segment-anything-2-real-time.git\n",
    "else:\n",
    "    print(\"Repo already cloned\")\n",
    "\n",
    "os.chdir(\"segment-anything-2-real-time\")\n",
    "print(\"CWD:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Install build dependencies in this env\n",
    "!pip install \"numpy<2\" ninja cmake setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Build the CUDA extension in-place\n",
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import site, os, pathlib\n",
    "\n",
    "repo_path = pathlib.Path(\"/home/valentinweyer/projects/handball-computer-vision/segment-anything-2-real-time\")\n",
    "site_dir = site.getsitepackages()[0]\n",
    "pth_path = pathlib.Path(site_dir) / \"segment_anything_2_real_time.pth\"\n",
    "\n",
    "with open(pth_path, \"w\") as f:\n",
    "    f.write(str(repo_path) + \"\\n\")\n",
    "\n",
    "print(\"Wrote .pth file:\", pth_path)\n",
    "print(\"Points to:\", repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/valentinweyer/projects/handball-computer-vision/segment-anything-2-real-time\n",
    "\n",
    "!pip install \\\n",
    "  \"numpy>=1.24.4,<2\" \\\n",
    "  \"tqdm>=4.66.1\" \\\n",
    "  \"hydra-core>=1.3.2\" \\\n",
    "  \"iopath>=0.1.10\" \\\n",
    "  \"pillow>=9.4.0,<12.0\" \\\n",
    "  \"torchvision>=0.18.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd checkpoints && bash download_ckpts.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiTmIfgBcc1e"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gdown\n",
    "!pip install supervision\n",
    "!pip install -q git+https://github.com/roboflow/sports.git@feat/basketball\n",
    "!pip install -q transformers num2words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGX Spark specific install\n",
    "I ran this on DGX Spark. There the aarch64 wheels for onnxruntime-gpu are not available yet, which is why I had to manually build it. The instructions are below. Skip, if you are not on DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "\n",
    "base = \"/home/valentinweyer/projects\" # give your base path here\n",
    "os.makedirs(base, exist_ok=True)\n",
    "os.chdir(base)\n",
    "\n",
    "if not pathlib.Path(\"onnxruntime\").exists():\n",
    "    !git clone --recursive https://github.com/microsoft/onnxruntime.git\n",
    "else:\n",
    "    print(\"onnxruntime repo already exists\")\n",
    "\n",
    "%cd onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the onnxruntime. This my takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh build.sh \\\n",
    "  --config Release \\\n",
    "  --build_dir build/cuda13 \\\n",
    "  --parallel 20 \\\n",
    "  --nvcc_threads 20 \\\n",
    "  --use_cuda \\\n",
    "  --cuda_version 13.0 \\\n",
    "  --cuda_home /usr/local/cuda-13.0/ \\\n",
    "  --cudnn_home /usr/local/cuda-13.0/ \\\n",
    "  --build_wheel \\\n",
    "  --skip_tests \\\n",
    "  --cmake_generator Ninja \\\n",
    "  --use_binskim_compliant_compile_flags \\\n",
    "  --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES=121 onnxruntime_BUILD_UNIT_TESTS=OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install build/cuda13/Release/dist/*.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps inference-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "  \"aiohttp>=3.9.0,<=3.10.11\" \\\n",
    "  \"aiortc>=1.9.0\" \\\n",
    "  \"anthropic~=0.49.0\" \\\n",
    "  \"APScheduler>=3.10.1,<4.0.0\" \\\n",
    "  \"asyncua~=1.1.5\" \\\n",
    "  \"backoff~=2.2.0\" \\\n",
    "  \"boto3<=1.35.60\" \\\n",
    "  \"cachetools<6.0.0\" \\\n",
    "  \"click<8.2.0\" \\\n",
    "  \"cython~=3.0.0\" \\\n",
    "  \"dataclasses-json~=0.6.0\" \\\n",
    "  \"docker>=7.0.0,<8.0.0\" \\\n",
    "  \"fastapi>=0.100,<0.116\" \\\n",
    "  \"httpx~=0.28.1\" \\\n",
    "  \"inference-exp>=0.15.2\" \\\n",
    "  \"nvidia-ml-py<13.0.0\" \\\n",
    "  \"onvif-zeep-async==2.0.0\" \\\n",
    "  \"openai>=1.12.0,<2.0.0\" \\\n",
    "  \"opencv-python>=4.8.1.78,<=4.10.0.84\" \\\n",
    "  \"opencv-contrib-python>=4.8.1.78,<=4.10.0.84\" \\\n",
    "  \"paho-mqtt~=1.6.1\" \\\n",
    "  \"pandas>=2.2.3,<3.0.0\" \\\n",
    "  \"prometheus-fastapi-instrumentator<=6.0.0\" \\\n",
    "  \"py-cpuinfo~=9.0.0\" \\\n",
    "  \"pybase64~=1.0.0\" \\\n",
    "  \"pydantic>=2.8.0,<2.12.0\" \\\n",
    "  \"pydantic-settings<2.8\" \\\n",
    "  \"pydot~=2.0.0\" \\\n",
    "  \"pylogix==1.0.5\" \\\n",
    "  \"pymodbus>=3.6.9,<=3.8.3\" \\\n",
    "  \"pytest>=8.0.0,<9.0.0\" \\\n",
    "  \"qrcode~=8.0.0\" \\\n",
    "  \"redis~=5.0.0\" \\\n",
    "  \"requests-toolbelt~=1.0.0\" \\\n",
    "  \"rich>=13.0.0,<15.0.0\" \\\n",
    "  \"scikit-image>=0.19.0,<=0.25.2\" \\\n",
    "  \"shapely>=2.0.4,<2.1.0\" \\\n",
    "  \"simple-pid~=2.0.1\" \\\n",
    "  \"slack-sdk~=3.33.4\" \\\n",
    "  \"structlog>=24.1.0,<25.0.0\" \\\n",
    "  \"tldextract~=5.1.2\" \\\n",
    "  \"twilio~=9.3.7\" \\\n",
    "  \"typer>=0.9.0,<=0.16.0\" \\\n",
    "  \"zxing-cpp~=2.2.0\" \\\n",
    "  \"filelock>=3.12.0,<=3.17.0\" \\\n",
    "  \"packaging~=24.0\" \\\n",
    "  \"pillow>=11.0,<12.0\" \\\n",
    "  \"python-dotenv~=1.0.0\" \\\n",
    "  \"typing_extensions>=4.8.0,<=4.12.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipped, \n",
    "# !pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General NVIDIA GPU installation\n",
    "Use this, if you have an NVIDIA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q inference-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6lzxw4HfRxJ"
   },
   "source": [
    "Set the ONNX Runtime execution provider to CUDA to ensure model inference runs on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWkLGOhnVpBG"
   },
   "source": [
    "### Source videos\n",
    "\n",
    "As an example, we will use sample videos from Game 1 of the 2025 NBA Playoffs between the Boston Celtics and the New York Knicks. We prepared 10 sample videos from this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_DIRECTORY = HOME / \"source\"\n",
    "\n",
    "#!gdown -q https://drive.google.com/drive/folders/1eDJYqQ77Fytz15tKGdJCMeYSgmoQ-2-H -O {SOURCE_VIDEO_DIRECTORY} --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la {SOURCE_VIDEO_DIRECTORY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SOURCE_VIDEO_PATH = SOURCE_VIDEO_DIRECTORY / \"FelixClaar.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsHR16EVFBoo"
   },
   "source": [
    "### Team rosters\n",
    "\n",
    "We are preparing player rosters for both teams. We load the official lists that link jersey numbers to player names. These mappings will let us replace detected numbers with real names, making the final analytics clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_ROSTERS = {\n",
    "  \"New York Knicks\": {\n",
    "    \"55\": \"Hukporti\",\n",
    "    \"1\": \"Payne\",\n",
    "    \"0\": \"Wright\",\n",
    "    \"11\": \"Brunson\",\n",
    "    \"3\": \"Hart\",\n",
    "    \"32\": \"Towns\",\n",
    "    \"44\": \"Shamet\",\n",
    "    \"25\": \"Bridges\",\n",
    "    \"2\": \"McBride\",\n",
    "    \"23\": \"Robinson\",\n",
    "    \"8\": \"Anunoby\",\n",
    "    \"4\": \"Dadiet\",\n",
    "    \"5\": \"Achiuwa\",\n",
    "    \"13\": \"Kolek\"\n",
    "  },\n",
    "  \"Boston Celtics\": {\n",
    "    \"42\": \"Horford\",\n",
    "    \"55\": \"Scheierman\",\n",
    "    \"9\": \"White\",\n",
    "    \"20\": \"Davison\",\n",
    "    \"7\": \"Brown\",\n",
    "    \"0\": \"Tatum\",\n",
    "    \"27\": \"Walsh\",\n",
    "    \"4\": \"Holiday\",\n",
    "    \"8\": \"Porzingis\",\n",
    "    \"40\": \"Kornet\",\n",
    "    \"88\": \"Queta\",\n",
    "    \"11\": \"Pritchard\",\n",
    "    \"30\": \"Hauser\",\n",
    "    \"12\": \"Craig\",\n",
    "    \"26\": \"Tillman\"\n",
    "  }\n",
    "}\n",
    "\n",
    "TEAM_COLORS = {\n",
    "    \"New York Knicks\": \"#006BB6\",\n",
    "    \"Boston Celtics\": \"#007A33\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-kTisAQYQHM"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from typing import Dict, List, Optional, Union, Iterable, Tuple\n",
    "from operator import itemgetter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "from sports import (\n",
    "    clean_paths,\n",
    "    ConsecutiveValueTracker,\n",
    "    TeamClassifier,\n",
    "    MeasurementUnit,\n",
    "    ViewTransformer\n",
    ")\n",
    "from sports.basketball import (\n",
    "    CourtConfiguration,\n",
    "    League,\n",
    "    draw_court,\n",
    "    draw_points_on_court,\n",
    "    draw_paths_on_court\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PupZT1xHhmoG"
   },
   "source": [
    "## Object detection\n",
    "\n",
    "The model used in this notebook detects the following classes: `ball`, `ball-in-basket`, `number`, `player`, `player-in-possession`, `player-jump-shot`, `player-layup-dunk`, `player-shot-block`, `referee`, and `rim`. These classes enable tracking of game events, player actions, and ball location for basketball analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5UNfe7fLvKf"
   },
   "source": [
    "### Load RF-DETR object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_DETECTION_MODEL_ID = \"player-and-handball-detection-3z9xf/1\"\n",
    "PLAYER_DETECTION_MODEL_CONFIDENCE = 0.4\n",
    "PLAYER_DETECTION_MODEL_IOU_THRESHOLD = 0.9\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID)\n",
    "\n",
    "COLOR = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
    "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMXt_1FxL2D-"
   },
   "source": [
    "### Single frame object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VzpR_LGEGad"
   },
   "source": [
    "### Keep only \"number\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_CLASS_ID = 4\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttZNW8qgPLBm"
   },
   "source": [
    "### Keep only player-related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_CLASS_IDS = [1, 2] # player, player-in-possession, player-jump-shot, player-layup-dunk, player-shot-block\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUZHIvfdNRi1"
   },
   "source": [
    "### Full video object detection\n",
    "\n",
    "We are running RF-DETR across all frames to produce a per-frame sequence of detections. These sequences seed tracking and provide number crops over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-detection{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-detection{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzfEx2GbQNuO"
   },
   "source": [
    "## Player tracking\n",
    "\n",
    "We are switching from frame-wise boxes to temporal tracks. SAM2 yields per-player masks and stable track IDs that persist through occlusions and re-entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6uqFLygS39e"
   },
   "source": [
    "### Load SAM2 tracking model\n",
    "\n",
    "We are loading a SAM2 checkpoint and config into the camera predictor. The large variant yields the highest quality masks; swap to smaller for speed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $HOME/segment-anything-2-real-time\n",
    "\n",
    "from sam2.build_sam import build_sam2_camera_predictor\n",
    "\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_camera_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYfdZW-nTg5N"
   },
   "source": [
    "### Full video player tacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csTvkOMoXZU4"
   },
   "source": [
    "We are prompting SAM2 with RF-DETR boxes and tracking across the clip. The callback saves masks, IDs, and visualizations for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class SAM2Tracker:\n",
    "    def __init__(self, predictor) -> None:\n",
    "        self.predictor = predictor\n",
    "        self._prompted = False\n",
    "\n",
    "    def prompt_first_frame(self, frame: np.ndarray, detections: sv.Detections) -> None:\n",
    "        if len(detections) == 0:\n",
    "            raise ValueError(\"detections must contain at least one box\")\n",
    "\n",
    "        if detections.tracker_id is None:\n",
    "            detections.tracker_id = list(range(1, len(detections) + 1))\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            self.predictor.load_first_frame(frame)\n",
    "            for xyxy, obj_id in zip(detections.xyxy, detections.tracker_id):\n",
    "                bbox = np.asarray([xyxy], dtype=np.float32)\n",
    "                self.predictor.add_new_prompt(\n",
    "                    frame_idx=0,\n",
    "                    obj_id=int(obj_id),\n",
    "                    bbox=bbox,\n",
    "                )\n",
    "\n",
    "        self._prompted = True\n",
    "\n",
    "    def propagate(self, frame: np.ndarray) -> sv.Detections:\n",
    "        if not self._prompted:\n",
    "            raise RuntimeError(\"Call prompt_first_frame before propagate\")\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            tracker_ids, mask_logits = self.predictor.track(frame)\n",
    "\n",
    "        tracker_ids = np.asarray(tracker_ids, dtype=np.int32)\n",
    "        masks = (mask_logits > 0.0).cpu().numpy()\n",
    "        masks = np.squeeze(masks).astype(bool)\n",
    "\n",
    "        if masks.ndim == 2:\n",
    "            masks = masks[None, ...]\n",
    "\n",
    "        masks = np.array([\n",
    "            sv.filter_segments_by_distance(mask, relative_distance=0.03, mode=\"edge\")\n",
    "            for mask in masks\n",
    "        ])\n",
    "\n",
    "        xyxy = sv.mask_to_xyxy(masks=masks)\n",
    "        detections = sv.Detections(xyxy=xyxy, mask=masks, tracker_id=tracker_ids)\n",
    "        return detections\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._prompted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-mask{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.5)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "**Handball specific only until here.**\n",
    "**Following code may not work properly.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNTHNNatcC_I"
   },
   "source": [
    "## Cluster players into teams\n",
    "\n",
    "EWe are assigning each track to a team without labels. The pipeline uses SigLIP embeddings, UMAP to 3D, then K-means with k=2 for final team IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx-Z3m9QhPIO"
   },
   "source": [
    "### Collecting training set\n",
    "\n",
    "We are sampling frames at 1 FPS, detecting players, and extracting central crops. Central regions emphasize jersey color and texture while reducing background artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIDE = 30\n",
    "\n",
    "crops = []\n",
    "\n",
    "for video_path in sv.list_files_with_extensions(SOURCE_VIDEO_DIRECTORY, extensions=[\"mp4\", \"avi\", \"mov\"]):\n",
    "    frame_generator = sv.get_video_frames_generator(source_path=video_path, stride=STRIDE)\n",
    "\n",
    "    for frame in tqdm(frame_generator):\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "        boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "        for box in boxes:\n",
    "            crops.append(sv.crop_image(frame, box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.plot_images_grid(\n",
    "    images=crops[:100],\n",
    "    grid_size=(10, 10),\n",
    "    size=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDIWnI9NmKI0"
   },
   "source": [
    "### Train and test clustering model\n",
    "\n",
    "We are computing SigLIP embeddings for crops, reducing with UMAP, and fitting K-means. A quick validation confirms separation by uniform appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_classifier = TeamClassifier(device=\"cuda\")\n",
    "team_classifier.fit(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = team_classifier.predict(crops)\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CdLxBkXKOca"
   },
   "source": [
    "### Test clustering model on single video frame\n",
    "\n",
    "We are applying the trained clustering to one frameâ€™s player crops. The output assigns provisional team IDs to confirm the mapping before full-video use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "teams = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMVitdaknPqa"
   },
   "source": [
    "Since we do not control which IDs the clustering algorithm assigns to the teams, after training and testing we must select one of the dictionaries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAMES = {\n",
    "    0: \"New York Knicks\",\n",
    "    1: \"Boston Celtics\",\n",
    "}\n",
    "\n",
    "# TEAM_NAMES = {\n",
    "#     0: \"Boston Celtics\",\n",
    "#     1: \"New York Knicks\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBKKa8MKo5j7"
   },
   "source": [
    "### Full video team clustering\n",
    "\n",
    "We are assigning team IDs to tracks once, then reusing them across frames via track IDs. This keeps colors and labels consistent throughout the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-teams{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = team_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    annotated_frame = team_box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0Q-gOy2su5v"
   },
   "source": [
    "## Player numbers OCR\n",
    "\n",
    "We are moving to jersey OCR to identify individuals within each team. Number reads pair with tracks and teams to resolve names later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXiZXnqntShC"
   },
   "source": [
    "### Load number recognition model\n",
    "\n",
    "We are loading the fine-tuned SmolVLM2 OCR model by ID. It was trained on jersey crops and outputs digit strings suitable for downstream validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_RECOGNITION_MODEL_ID = \"basketball-jersey-numbers-ocr/3\"\n",
    "NUMBER_RECOGNITION_MODEL = get_model(model_id=NUMBER_RECOGNITION_MODEL_ID)\n",
    "NUMBER_RECOGNITION_MODEL_PROMPT = \"Read the number.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkM_JQ61s3Oc"
   },
   "source": [
    "### Single frame player number detection and recognition\n",
    "\n",
    "We are detecting number boxes, padding, and cropping. We then run SmolVLM2 on each crop and preview predictions next to the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = [\n",
    "    sv.resize_image(sv.crop_image(frame, xyxy), resolution_wh=(224, 224))\n",
    "    for xyxy\n",
    "    in sv.clip_boxes(sv.pad_boxes(xyxy=detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "]\n",
    "numbers = [\n",
    "    NUMBER_RECOGNITION_MODEL.predict(crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "    for crop\n",
    "    in crops\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=crops[:10],\n",
    "    titles=numbers[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3fw2eXcuyVA"
   },
   "source": [
    "### Single frame player detection with number detection matching\n",
    "\n",
    "We are matching numbers to players using Intersection over Smaller Area. IoS equals 1.0 implies the number lies fully inside the player mask, so we link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_above_threshold(\n",
    "    matrix: np.ndarray, threshold: float, sort_desc: bool = True\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return all (row_index, col_index) where value > threshold.\n",
    "    Rows and columns may repeat.\n",
    "    Optionally sort by value descending.\n",
    "    \"\"\"\n",
    "    A = np.asarray(matrix)\n",
    "    rows, cols = np.where(A > threshold)\n",
    "    pairs = list(zip(rows.tolist(), cols.tolist()))\n",
    "    if sort_desc:\n",
    "        pairs.sort(key=lambda rc: A[rc[0], rc[1]], reverse=True)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=4, color_lookup=sv.ColorLookup.TRACK)\n",
    "\n",
    "player_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(3), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "number_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(0), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "\n",
    "    # we only process the first video frame\n",
    "\n",
    "    if index > 0:\n",
    "        break\n",
    "\n",
    "    frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use RF-DETR model to detect numbers\n",
    "\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    number_detections = sv.Detections.from_inference(result)\n",
    "    number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "    number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "    # we use mask IoS to match numbers with players\n",
    "\n",
    "    iou = sv.mask_iou_batch(\n",
    "        masks_true=player_detections.mask,\n",
    "        masks_detection=number_detections.mask,\n",
    "        overlap_metric=sv.OverlapMetric.IOS\n",
    "    )\n",
    "\n",
    "    pairs = coords_above_threshold(iou, 0.9)\n",
    "    player_idx, number_idx = zip(*pairs)\n",
    "\n",
    "    # we visualize all the masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = player_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = number_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    sv.plot_image(annotated_frame)\n",
    "\n",
    "    # we visualize only matched pairs\n",
    "\n",
    "    player_detections = player_detections[np.array(player_idx)]\n",
    "    number_detections = number_detections[np.array(number_idx)]\n",
    "    number_detections.tracker_id = player_detections.tracker_id\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqaXSQBVB5kV"
   },
   "source": [
    "### Validating recognized numbers\n",
    "\n",
    "We are confirming numbers across time using a consecutive-agreement threshold. The validator locks a number to a track only after repeated consistent reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-validated-numbers{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.7)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=0.8)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)\n",
    "\n",
    "    # we visualize boxes and masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "\n",
    "    # we extract validated numbers\n",
    "\n",
    "    numbers = number_validator.get_validated(tracker_ids=player_detections.tracker_id)\n",
    "\n",
    "    # we visualize numbers\n",
    "\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections,\n",
    "        labels=numbers)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp9TzWCZOmr9"
   },
   "source": [
    "## Player recognition\n",
    "\n",
    "We are overlaying names, numbers, team colors, and masks for each tracked player. The final render shows stable identities aligned with roster data across the full clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/drive/folders/1RBjpI5Xleb58lujeusxH0W5zYMMA4ytO -O {HOME / \"fonts\"} --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_history = []\n",
    "detections_history = []\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "team_validator = ConsecutiveValueTracker(n_consecutive=1)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_validator.update(tracker_ids=detections.tracker_id, values=TEAMS)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "    player_detections = tracker.propagate(frame)\n",
    "    frames_history.append(frame)\n",
    "    detections_history.append(player_detections)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-result{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "team_label_annotator = sv.RichLabelAnnotator(\n",
    "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
    "    font_size=40,\n",
    "    color=team_colors,\n",
    "    text_color=sv.Color.WHITE,\n",
    "    text_position=sv.Position.BOTTOM_CENTER,\n",
    "    text_offset=(0, 10),\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame, detections in tqdm(zip(frames_history, detections_history)):\n",
    "        detections = detections[detections.area > 100]\n",
    "\n",
    "        teams = team_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        teams = np.array(teams).astype(int)\n",
    "        numbers = number_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        numbers = np.array(numbers)\n",
    "\n",
    "        labels = [\n",
    "            f\"#{number} {TEAM_ROSTERS[TEAM_NAMES[team]].get(number)}\"\n",
    "            for number, team\n",
    "            in zip(numbers, teams)\n",
    "        ]\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = team_mask_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            custom_color_lookup=teams)\n",
    "        annotated_frame = team_label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels,\n",
    "            custom_color_lookup=teams)\n",
    "\n",
    "        sink.write_frame(annotated_frame)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-YW3b5Qi-U6"
   },
   "source": [
    "## Court keypoints detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsKMDgdqjmnI"
   },
   "source": [
    "### Load keypoint detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINT_DETECTION_MODEL_ID = \"basketball-court-detection-2/14\"\n",
    "KEYPOINT_DETECTION_MODEL_CONFIDENCE = 0.3\n",
    "KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE = 0.5\n",
    "KEYPOINT_DETECTION_MODEL = get_model(model_id=KEYPOINT_DETECTION_MODEL_ID)\n",
    "KEYPOINT_COLOR = sv.Color.from_hex('#FF1493')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqchilTtkWz6"
   },
   "source": [
    "### Single frame keypoint detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCcYa07Dk-Lf"
   },
   "source": [
    "### Detecting keypoints with high confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "key_points = key_points[:, key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B_KM5MNlYBm"
   },
   "source": [
    "## Map player positions to court coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdbDdYc10Pb-"
   },
   "source": [
    "## Single frame player position mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = team_box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections,\n",
    "    custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    ")\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we use a keypoint model to detect court landmarks\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "    # we calculate homography matrix\n",
    "\n",
    "    court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "    frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "    frame_to_court_transformer = ViewTransformer(\n",
    "        source=frame_landmarks,\n",
    "        target=court_landmarks,\n",
    "    )\n",
    "\n",
    "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    if len(frame_xy) > 0:\n",
    "\n",
    "        # we transform points\n",
    "\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "\n",
    "        # we visualize the results\n",
    "\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sv.plot_image(court)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJiEzaE85jmW"
   },
   "source": [
    "### Full video player position mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_xy = []\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for frame_idx, frame in tqdm(enumerate(frame_generator)):\n",
    "    detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use a keypoint model to detect court landmarks\n",
    "\n",
    "    result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "    key_points = sv.KeyPoints.from_inference(result)\n",
    "    landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "    if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "        # we calculate homography matrix\n",
    "\n",
    "        court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "        frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "        frame_to_court_transformer = ViewTransformer(\n",
    "            source=frame_landmarks,\n",
    "            target=court_landmarks,\n",
    "        )\n",
    "\n",
    "        frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "        video_xy.append(court_xy)\n",
    "\n",
    "video_xy = np.array(video_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(video_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUhD1VKNM41D"
   },
   "source": [
    "### Clean player movement paths\n",
    "\n",
    "We are detecting sudden jumps in position using robust speed analysis. We are removing short abnormal runs and nearby frames to eliminate teleport-like artifacts. We are filling missing segments with linear interpolation to ensure continuous motion. We are smoothing all paths with a Savitzkyâ€“Golay filter to achieve stable and natural movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "sv.plot_image(court)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_xy, edited_mask = clean_paths(\n",
    "    video_xy,\n",
    "    jump_sigma=3.5,\n",
    "    min_jump_dist=0.6,\n",
    "    max_jump_run=18,\n",
    "    pad_around_runs=2,\n",
    "    smooth_window=9,\n",
    "    smooth_poly=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_true_runs(mask: np.ndarray, coords: np.ndarray) -> list[np.ndarray]:\n",
    "    mask = mask.squeeze()\n",
    "    idx = np.flatnonzero(mask)\n",
    "    if idx.size == 0:\n",
    "        return []\n",
    "    splits = np.where(np.diff(idx) > 1)[0] + 1\n",
    "    groups = np.split(idx, splits)\n",
    "    return [coords[g, 0, :] for g in groups]\n",
    "\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    "    color=sv.Color.GREEN,\n",
    ")\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=split_true_runs(edited_mask[:, 0], video_xy),\n",
    "    color=sv.Color.RED,\n",
    "    court=court\n",
    ")\n",
    "\n",
    "sv.plot_image(court)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[cleaned_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "sv.plot_image(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(cleaned_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARvSRBTLaZXD"
   },
   "source": [
    "<div align=\"center\">\n",
    "  <p>\n",
    "    Looking for more tutorials or have questions?\n",
    "    Check out our <a href=\"https://github.com/roboflow/notebooks\">GitHub repo</a> for more notebooks,\n",
    "    or visit our <a href=\"https://discord.gg/GbfgXGJ8Bk\">discord</a>.\n",
    "  </p>\n",
    "  \n",
    "  <p>\n",
    "    <strong>If you found this helpful, please consider giving us a â­\n",
    "    <a href=\"https://github.com/roboflow/notebooks\">on GitHub</a>!</strong>\n",
    "  </p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
